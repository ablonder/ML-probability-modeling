{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Bernoulli MLE.\n",
    "A dataset of $ N $ binary values, $ x_1, ..., x_N $, was sampled from a Bernoulli distribution with success parameter $ q $.\n",
    "\n",
    "## a) Write out the likelihood function for $ q $.\n",
    "$$ L(q; X) = p(X; q) $$\n",
    "Because $ X $ is a random vector composed of independant random variables $ x_1, ..., x_N $\n",
    "$$ p(X | q) = \\prod_{n=1}^N p(x_n | q) $$\n",
    "Each value of $ x $ is drawn from a Bernoulli distribution with parameter $ q $, so\n",
    "$$ p(x | q) = q^x(1-q)^{1-x} \\hspace{1 cm} x = 1, 0 $$\n",
    "Putting that all together\n",
    "$$ L(q; X) = \\prod_{n=1}^N q^{x_n}(1-q)^{1-x_n} \\hspace{.5 cm} x_n = 1, 0 $$\n",
    "\n",
    "## b) Find a formula for the Maximum Likelihood Estimator.\n",
    "To maximize $ L(q; X) $, we can set the derivative of $ L $ to 0 and solve for $ q $.\n",
    "But first, to simplify things, we will take the natural log of $ L(q; X) $, which will result in the same maximum, but somewhat easier computations.\n",
    "$$ ln(L(q; X)) = \\sum_{n=1}^N \\bigg(x_nln(q) + (1-x_n)ln(1-q)\\bigg) $$\n",
    "Then we can take the derivative of $ ln(L) $\n",
    "$$ \\frac{\\partial ln(L)}{\\partial q} = \\sum_{n=1}^N \\bigg( \\frac{x_n}{q} - \\frac{1-x_n}{1-q} \\bigg) $$\n",
    "If we set the derivative equal to 0, we can solve for the estimated value of $ q $\n",
    "$$ 0 =  \\sum_{n=1}^N \\bigg( \\frac{x_n(1-\\hat{q}) - \\hat{q}(1-x_n)}{\\hat{q}(1-\\hat{q})} \\bigg)\n",
    " = \\sum_{n=1}^N \\bigg( \\frac{x_n - \\hat{q}}{\\hat{q}(1-\\hat{q})} \\bigg)$$\n",
    "For a fraction to be equal to zero, the numerator must be equal to zero, so that simplifies to\n",
    "$$ 0 = \\sum_{n=1}^N (x_n - \\hat{q}) $$\n",
    "Adding the estimated value of $ q $ times $ N $ to both sides, we get\n",
    "$$ N\\hat{q} = \\sum_{n=1}^N x_n $$\n",
    "If we divide by $ N $, we find that\n",
    "$$ \\hat{q} = \\frac{1}{N} \\sum_{n=1}^N x_n $$\n",
    "\n",
    "\n",
    "# 2. Univariate Normal (Gaussian) MLE.\n",
    "A dataset of $ N $ real-valued observations was generated by a Normal distribution with a mean of mu and varriance of sigma squared.\n",
    "\n",
    "## a) Write out the likelihood function for mu and sigma squared based on the dataset.\n",
    "$$ L(\\mu, \\sigma^2; X) = p(X; \\mu, \\sigma^2) $$\n",
    "Because $ X $ is a random vector composed of independant random variables\n",
    "$$ p(X; \\mu, \\sigma^2) = \\prod_{n=1}^N p(x_n; \\mu, \\sigma^2) $$\n",
    "Each value in $ X $ was drawn from a normal distribution, so\n",
    "$$ p(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$\n",
    "Putting that all together\n",
    "$$ L(\\mu, \\sigma^2; X) = \\prod_{n=1}^N \\frac{1}{\\sqrt{2\\sigma^2\\pi}}e^{-\\frac{(x_n-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "## b) Find the maximum likelihood estimate of the mean.\n",
    "To maximize the likelihood of mu and sigma squared, we can take the partial derivative of $ L $ in terms of each of them. But first we will take the natural log of $ L $ which has the same maximum, but enables it to be computed more easily.\n",
    "$$ ln(L(\\mu, \\sigma^2; X)) = \\sum_{n=1}^N \\bigg( ln(1) - \\frac{1}{2}\\big(ln(2\\pi) + ln(\\sigma^2)\\big) - \\frac{(x_n-\\mu)^2}{2\\sigma^2} \\bigg) $$\n",
    "Now we can take the partial derivative of $ ln(L) $ in terms of mu.\n",
    "$$ \\frac{\\partial ln(L)}{\\partial \\mu} = \\sum_{n=1}^N \\bigg( \\frac{x_n-\\mu}{\\sigma^2} \\bigg) $$\n",
    "To find the maximum likelihood, we then set the partial derivative equal to zero and solve for mu. For a fraction to be equal to zero, the numerator must be equal to zero, so it simplifies to\n",
    "$$ 0 = \\sum_{n=1}^N (x_n - \\hat{\\mu}) $$\n",
    "Adding the estimated value of mu times $ N $ to both sides, and dividing by $ N $, we find that\n",
    "$$ \\hat{\\mu} = \\frac{1}{N} \\sum_{n=1}^N x_n $$\n",
    "\n",
    "## Find the maximum likelihood estimate of the variance.\n",
    "We can find the most likely variance the same way. First we take the partial derivative of $ ln(L) $ in terms of sigma squared:\n",
    "$$ \\frac{\\partial ln(L)}{\\partial \\sigma^2} = \\sum_{n=1}^N \\bigg( \\frac{(x_n-\\mu)^2}{2(\\sigma^2)^2} -\\frac{1}{\\sigma^2} \\bigg) $$\n",
    "Now we can set the partial derivative equal to zero and solve for sigma squared. For a fraction to be equal to zero, the numerator must be equal to zero, so it simplifies to\n",
    "$$ 0 = \\sum_{n=1}^N \\bigg( (x_n-\\mu)^2 - 2\\hat{\\sigma}^2 \\bigg) $$\n",
    "Adding the estimated value of sigma squared times $ 2N $ to both sides and dividing by $ 2N $ we get\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{2N} \\sum_{n=1}^N (x_n-\\mu)^2 $$\n",
    "To calculate sigma squared purely in terms of $ X $, we can substitute mu for its estimated value, calculated above\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{2N} \\sum_{n=1}^N \\bigg( x_n - \\frac{1}{N} \\sum_{n=1}^N x_n \\bigg)^2 $$\n",
    "Which is to say that the variance is one half the average squared difference between each value of $ X $ and the mean value of $ X $.\n",
    "\n",
    "\n",
    "# 3. MLE of Noise Variance in Linear Regression.\n",
    "The MLE of the noise variance in a linear model is given as\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{N} (t^{\\top}t - t^{\\top}X\\hat{w}) $$\n",
    "Show that it can also be written as\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{n=1}^N (t_n - x_n^{\\top}\\hat{w})^2 $$\n",
    "We will begin by multiplying out the second equation and distributing the sigma.\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{N} \\bigg(\\sum_{n=1}^N t_n^2 - \\sum_{n=1}^N 2t_nx_n^{\\top}\\hat{w} + \\sum_{n=1}^N (x_n^{\\top}\\hat{w})^2 \\bigg) $$\n",
    "\n",
    "$$ \\sum_{n=1}^N t_n^2 = t^{\\top}t $$ and\n",
    "$$ \\sum_{n=1}^N t_nx_n^{\\top} = t^{\\top}X $$\n",
    "So we are left with\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{N} \\bigg( t^{\\top}t - 2t^{\\top}X\\hat{w} + \\sum_{n=1}^N (x_n^{\\top}\\hat{w})^2 \\bigg) $$\n",
    "\n",
    "$$ \\hat{w} = (X^{\\top}X)^{-1}X^{\\top}t $$\n",
    "Therefore,\n",
    "$$ (x_n^{\\top}\\hat{w})^2 = x_n^{\\top}(X^{\\top}X)^{-1}X^{\\top}tx_n^{\\top}\\hat{w} $$\n",
    "Because\n",
    "$$ \\sum_{n=1}^N t_nx_n^{\\top} = t^{\\top}X $$\n",
    "We get\n",
    "$$ \\bigg( \\sum_{n=1}^N x_n^{\\top}(X^{\\top}X)^{-1}X^{\\top} \\bigg) t^{\\top}X\\hat{w} = X(X^{\\top}X)^{-1}X^{\\top}t^{\\top}X\\hat{w} = t^{\\top}X\\hat{w} $$\n",
    "We can then plug that in\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{N} \\bigg( t^{\\top}t - 2t^{\\top}X\\hat{w} + t^{\\top}X\\hat{w} \\bigg) = \\frac{1}{N} \\bigg( t^{\\top}t - t^{\\top}X\\hat{w} \\bigg) $$\n",
    "Which is what we intended to show.\n",
    "\n",
    "\n",
    "# 4. MLE for Linear Regression With Non-Constant Noise Variance.\n",
    "Show that the weight vector that maximizes the log likelihood also minimizes the weighted least squares loss for a particular set of weight values - find that set of weight values.\n",
    "\n",
    "The likelihood of a weight vector and parameter matrix given the data is equal to the probability of the data given a weight vector and parameter matrix. In this case, the noise is drawn from a normal distribution, so if we assume all the data points are independent\n",
    "$$ p(X, t; w, \\Sigma) = \\prod_{n=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2_n}}e^{-\\frac{(t_n-x_nw)^2}{2\\sigma^2_n}} $$\n",
    "To find the weight vector that maximizes the likelihood, we will first take its natural log\n",
    "$$ ln(L) = \\sum_{n=1}^N \\bigg( ln(1) - \\frac{1}{2}ln(2 \\pi \\sigma^2_n) - \\frac{(t_n-x_nw)^2}{2\\sigma^2_n} \\bigg) $$\n",
    "And then take the partial derivative of the log likelihood over the weight vector\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\sum_{n=1}^N x_n \\frac{(t_n-x_nw)}{\\sigma^2_n} $$\n",
    "If we set that equal to zero and solve for $ w $, we get:\n",
    "$$ 0 = \\sum_{n=1}^N \\frac{x_nt_n-x_nx_n\\hat{w}}{\\sigma^2_n} $$\n",
    "The sum can be rewritten as the multiplication of matricies, where Sigma is sigma squared times the identity matrix.\n",
    "$$ 0 = (Xt - X^{\\top}X\\hat{w})\\Sigma^{-1} = Xt\\Sigma^{-1} - X^{\\top}X\\hat{w}\\Sigma^{-1} $$\n",
    "Add the component containing the weght vector to both sides to get\n",
    "$$ X^{\\top}X\\hat{w}\\Sigma^{-1} = Xt\\Sigma^{-1} $$\n",
    "Then multiply both sides by\n",
    "$$ (X^{\\top}X\\Sigma^{-1})^{-1} $$\n",
    "To get\n",
    "$$ \\hat{w} = Xt\\Sigma^{-1}(X^{\\top}X\\Sigma^{-1})^{-1} $$\n",
    "\n",
    "Weighted least squares loss is\n",
    "$$ \\mathcal{L}(w; x, t) = \\frac{1}{N} \\sum_{n=1}^N \\alpha_n (t_n - w^{\\top}x_n)^2 $$\n",
    "To find the weight vector that minimizes loss, we take the partial derivative of the loss over the weight vector\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w} = -\\frac{2}{N} \\sum_{n=1}^N \\alpha_n (t_n - w^{\\top}x_n)x_n $$\n",
    "We can then set that equal to zero and solve for $ w $:\n",
    "$$ 0 = -\\frac{2}{N} \\sum_{n=1}^N (\\alpha_nt_nx_n - \\alpha_n\\hat{w}^{\\top}x_nx_n) $$\n",
    "$$ \\frac{2}{N} \\sum_{n=1}^N \\alpha_n\\hat{w}^{\\top}x_nx_n = \\frac{2}{N} \\sum_{n=1}^N \\alpha_nt_nx_n $$\n",
    "We will multiply both sides by $ N/2 $ and convert the sums into matricies, where $ A $ is alpha times the identity matrix\n",
    "$$ A\\hat{w}^{\\top}XX^{\\top} = AtX $$\n",
    "Now we multiply both sides by\n",
    "$$ (AXX^{\\top})^{-1} $$\n",
    "And transpose both sides to get\n",
    "$$ \\hat{w} = (AtX)^{\\top}(A^{\\top}X^{\\top}X)^{-1} $$\n",
    "This is the same as the equation for the weight vector as calculated above, except with A instead of the inverse of Sigma. That implies that the suitable choice for alpha is one over the variance.\n",
    "\n",
    "\n",
    "# 5. Bayesian inference for a proportion.\n",
    "A magazine interviews $ n $ people from its target audience, $ Y $ of them have seen the latest issue. Out of their target audience as a whole, $ q $ of them have seen the latest issue.\n",
    "\n",
    "## a) Using a uniform prior on $ q $, find the posterior distribution of $ q $ in terms of $ y $ and $ n $.\n",
    "$$ p(q | y, n) = \\frac{p(q)p(y|q)}{\\int_0^1 p(q')p(y|q')} $$\n",
    "Because we used a uniform prior on $ q $, $ p(q) = 1 $.\n",
    "$ y $ can be drawn from a binomial distribution\n",
    "$$ p(y|q) = (^n_y) q^y(1-q)^{n-y} $$\n",
    "This makes the posterior\n",
    "$$ p(q | y, n) = \\frac{(^n_y) q^y(1-q)^{n-y}}{\\int_0^1 (^n_y) q'^y(1-q')^{n-y}} $$\n",
    "\n",
    "## b) Find the expected value of $ q $ given a value of $ Y $ in terms of $ y $ and $ n $.\n",
    "$$ \\mathbb{E}[q|Y=y] = \\int_0^1 q p(q|y) $$\n",
    "As shown above\n",
    "$$ p(q | y, n) = \\frac{(^n_y) q^y(1-q)^{n-y}}{\\int_0^1 (^n_y) q'^y(1-q')^{n-y}} $$\n",
    "So\n",
    "$$ \\mathbb{E}[q|Y=y] = \\frac{1}{\\int_0^1 (^n_y) q'^y(1-q')^{n-y}} \\int_0^1 q(^n_y) q^y(1-q)^{n-y} $$ \n",
    "\n",
    "## c) Rewrite the expected value of $ q $ given the data as a weighted average of the MLE of $ q $ and the prior mean (the expected value of $ q $).\n",
    "In other words, we need to find a value of alpha such that\n",
    "$$ \\frac{1}{\\int_0^1 (^n_y) q'^y(1-q')^{n-y}} \\int_0^1 q(^n_y) q^y(1-q)^{n-y} = \\alpha\\mathbb{E}[q] + (1-\\alpha)\\hat{q} $$\n",
    "To do so, we must first find the expected value of $ q $ and the MLE of $ q $.\n",
    "$$ \\mathbb{E}[q] = \\int_0^1 q p(q) = .5 $$\n",
    "To find the MLE, we must maximize\n",
    "$$ L(q; n, y) = p(y; q, n) = (^n_y) q^y(1-q)^{n-y} $$\n",
    "\n",
    "Maximizing the log likelihood is the same as maximizing the likelihood, so first we'll take the log to simplify calculations later\n",
    "\n",
    "$$ log(L) = log((^n_y)) + ylog(q) + (n-y)log(1-q) $$\n",
    "Now we can take the partial derivative of $ log(L) $ over $ q $\n",
    "$$ \\frac{\\partial log(L)}{\\partial q} = \\frac{y}{q} + \\frac{n-y}{1-q} $$\n",
    "Then we can set that equal to zero and solve for q\n",
    "$$ 0 = \\frac{y(1-\\hat{q}) + \\hat{q}(n-y)}{\\hat{q}(1-\\hat{q})} $$\n",
    "If a fraction is equal to zero, the numerator must be equal to zero, so we can ignore the denominator, leaving us with\n",
    "$$ 0 = y-2y\\hat{q} + n\\hat{q} $$\n",
    "If we move the $ q $s to the other side, we get\n",
    "$$ \\hat{q}(2y - n) = y $$\n",
    "\n",
    "$$ \\hat{q} = \\frac{y}{2y - n} $$\n",
    "\n",
    "Putting that all together we get\n",
    "$$ \\frac{1}{\\int_0^1 (^n_y) q'^y(1-q')^{n-y}} \\int_0^1 q(^n_y) q^y(1-q)^{n-y} = \\alpha*.5 + (1-\\alpha)\\frac{y}{2y - n} $$\n",
    "Now we must solve for alpha.\n",
    "$$ \\alpha*.5 + (1-\\alpha)\\frac{y}{2y - n} = \\alpha \\bigg( \\frac{1}{2} - \\frac{y}{2y - n} \\bigg) + \\frac{y}{2y - n} $$\n",
    "We can now subtract the part that doesn't include alpha from both sides and divide by its coefficient, resulting in\n",
    "$$ \\alpha = \\frac{4y - 2n}{-n \\int_0^1 (^n_y) q'^y(1-q')^{n-y} } \\int_0^1 q(^n_y) q^y(1-q)^{n-y} - \\frac{2y}{n} $$\n",
    "\n",
    "## d) Show that the uniform prior from 0 to 1 is a special case of a Beta distribution.\n",
    "In the uniform distribution we used above\n",
    "$$ p(x) = \\begin{cases} \\frac{1}{1-0} \\hspace{.5 cm} a \\leq x \\leq b \\\\ 0 \\hspace{1 cm} otherwise \\end{cases} $$\n",
    "The probability density function of a Beta function is\n",
    "$$ p(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1 - x)^{\\beta-1} $$\n",
    "This is equal to one - as is the case in a uniform prior - if both alpha and beta are equal to 1.\n",
    "\n",
    "## and generalize the equation for the expected value of $ q $ written as a weighted average of the MLE and the prior mean for a prior with arbitrary parameters.\n",
    "First, we must find the expected value of $ q $ given a value of $ Y $.\n",
    "$$ \\mathbb{E}[q|Y=y] = \\int_0^1 q p(q|y) $$\n",
    "To find the expected value, first we need the probability of $ q $ given values of $ y $ and $ n $\n",
    "$$ p(q|y, n) = \\frac{p(q)p(y|q)}{\\int_0^1 p(q')p(y|q')} $$\n",
    "If we draw $ q $ from a Beta distribution, the prior on $ q $ is\n",
    "$$ p(q) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}q^{\\alpha-1}(1 - q)^{\\beta-1} $$\n",
    "$ y $ is still drawn from a Binomial distribution, so $ p(y|q) $ is the same as before:\n",
    "$$ p(y|q) = (^n_y) q^y(1-q)^{n-y} $$\n",
    "That makes the probability of $ q $\n",
    "$$ p(q|y, n) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\frac{q^{\\alpha-1}(1 - q)^{\\beta-1} (^n_y) q^y(1-q)^{n-y}}{\\int_0^1 q'^{\\alpha-1}(1 - q')^{\\beta-1} (^n_y) q'^y(1-q')^{n-y}} $$\n",
    "Now we can plug all of this into the equation for the expected value of $ q $ to get\n",
    "$$ \\mathbb{E}[q|Y=y] = \\frac{1}{\\int_0^1 q'^{\\alpha-1}(1 - q')^{\\beta-1} (^n_y) q'^y(1-q')^{n-y}} \\int_0^1 q q^{\\alpha-1}(1 - q)^{\\beta-1} (^n_y) q^y(1-q)^{n-y} $$\n",
    "\n",
    "Next, we must find the MLE of $ q $ and the prior mean.\n",
    "The prior mean is the expected value of $ q $\n",
    "$$ \\mathbb{E}[q] = \\int_0^1 qp(q) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1 q q^{\\alpha-1}(1 - q)^{\\beta-1} $$\n",
    "We find the MLE of $ q $ by maximizing its likelihood\n",
    "$$ L(q; y, n) = p(y; q, n) = (^n_y) q^y(1-q)^{n-y} $$\n",
    "This is the same as before where we showed that\n",
    "$$ \\hat{q} = \\frac{y}{2y - n} $$\n",
    "\n",
    "Finally, we can find the value of $ x $ that makes\n",
    "$$ \\frac{1}{\\int_0^1 q'^{\\alpha-1}(1 - q')^{\\beta-1} (^n_y) q'^y(1-q')^{n-y}} \\int_0^1 q q^{\\alpha-1}(1 - q)^{\\beta-1} (^n_y) q^y(1-q)^{n-y} = x \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1 q q^{\\alpha-1}(1 - q)^{\\beta-1} + (1-x)\\frac{y}{2y - n} $$\n",
    "Reorganizing terms on the right, we get\n",
    "$$ x \\bigg( \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1 q q^{\\alpha-1}(1 - q)^{\\beta-1} - \\frac{y}{2y - n} \\bigg) + \\frac{y}{2y - n} $$\n",
    "Now we can subtract $ y $ over $ 2y - n $ and divide by the coefficient on the $ x $ to get\n",
    "$$ x = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} \\frac{1}{\\int_0^1 q q^{\\alpha-1}(1 - q)^{\\beta-1} - \\frac{y}{2y - n}} \\bigg( \\frac{1}{\\int_0^1 q'^{\\alpha-1}(1 - q')^{\\beta-1} (^n_y) q'^y(1-q')^{n-y}} \\int_0^1 q q^{\\alpha-1}(1 - q)^{\\beta-1} (^n_y) q^y(1-q)^{n-y} - \\frac{y}{2y - n} \\bigg) $$\n",
    "\n",
    "## e) What does the expression for alpha (or $ x $) suggest about the interpretation of the prior parameters?\n",
    "They have a significant influence on how the prior and the data are weighted in the posterior. The larger they are the more strongly the prior is weighted, whereas the smaller they are the more strongly the data is weighted. This is intuitive because the stronger the parameters the stronger the prior, and stronger priors are favored.\n",
    "\n",
    "\n",
    "# 6. Inferring a Detection Limit.\n",
    "$ Y $ is a non-negative measurement that is uniformly distributed and can only be detected up to an unknown threshold.\n",
    "\n",
    "## a) Find a formula for the likelihood function of the threshold given a single observation $ Y $.\n",
    "$$ L(\\theta; Y) = p(Y|\\theta) = \\frac{1}{\\theta} \\hspace{1 cm} 0 \\leq Y \\leq \\theta $$\n",
    "## Where is it largest?\n",
    "It approaches infinity as the threshold approaches zero. That is because the closer to zero the threshold gets, the more probable any value between zero and the threshold can be for the probability of the sample space to still integrate to 1.\n",
    "\n",
    "## b) Find the posterior range for the threshold given an observation $ Y = y $.\n",
    "For $ Y $ to be detected, its value must be below the threshold. Therefore, if we detect a value $ y $, the threshold must be greater than or equal to $ y $, or else it would not have been detected. This makes the posterior range for the threshold after a single observation $ [y, \\infty) $.\n",
    "\n",
    "## c) Find the non-constant component of the posterior if the threshold has a Gamma prior with density\n",
    "$$ p(\\theta) = \\begin{cases} \\frac{b^a}{\\Gamma(a)} \\theta^{a-1}e^{-b\\theta} \\hspace{.5 cm} \\theta > 0 \\\\\n",
    "0 \\hspace{2 cm} otherwise \\end{cases} $$\n",
    "The posterior is\n",
    "$$ p(\\theta | Y=y) = \\frac{p(\\theta)p(y|\\theta)}{\\int_0^1 p(\\theta') p(y | \\theta')} $$\n",
    "The whole denominator is a constant, so we are only interested in the numerator\n",
    "$$ p(\\theta)p(y|\\theta) = \\frac{b^a}{\\Gamma(a)} \\theta^{a-1}e^{-b\\theta} * \\frac{1}{\\theta} = \\frac{b^a}{\\Gamma(a)} \\theta^{a-2}e^{-b\\theta} $$\n",
    "## Is the posterior also a Gamma distribution? How do you know?\n",
    "Yes, because it has the same form. In fact the only difference between them is that theta is raised to $ a - 2 $ in the posterior, as opposed to $ a - 1 $ in the prior. The parameters interact and are grouped in largely the same way.\n",
    "\n",
    "\n",
    "# 7. A waiting time model.\n",
    "The exponential distribution with rate parameter λ and range [0,∞) is often used to model the amount of time that passes between two events. Its density is given by\n",
    "$$ p(y|\\lambda) = \\lambda e^{−\\lambda y} $$\n",
    "\n",
    "## a) This is a special case of a Gamma($ a, b $)  density. Find the values of $ a $ and $ b $ that make the densities equivalent.\n",
    "The two densities are equivalent if we take $ a $ to be one and $ b $ to be lambda. In that case we get\n",
    "$$ p(y) = \\frac{\\lambda^1}{\\Gamma(1)} y^{1-1}e^{-\\lambda y} = \\lambda e^{−\\lambda y} $$\n",
    "\n",
    "## b) Show that the Gamma family is also a conjugate prior for the rate parameter.\n",
    "The prior density on lambda has the form\n",
    "$$ f(\\lambda) = k_{a, b} \\lambda^{a-1}e^{-b\\lambda} $$\n",
    "Then the posterior on lambda is\n",
    "$$ p(\\lambda|y) = \\frac{p(\\lambda)p(y|\\lambda)}{\\int_0^1 p(\\lambda')p(y|\\lambda')} = \\frac{k_{a, b} \\lambda^{a-1}e^{-b\\lambda} \\lambda e^{−\\lambda y}}{\\int_0^1 p(\\lambda')p(y|\\lambda')} $$\n",
    "\n",
    "The denominator is a constant, so it can be lumped in to $ k' $ with $ k_{a, b} $ We are left with\n",
    "\n",
    "$$ p(\\lambda|y) = k' \\lambda^{a-1}e^{-b\\lambda} \\lambda e^{−\\lambda y} = k' \\lambda^{a}e^{-\\lambda(b+y)} $$\n",
    "If we then set $ a' $ equal to $ a + 1 $ and $ b' $ equal to $ b + y $, we get\n",
    "$$ p(\\lambda|y) = k' \\lambda^{a'-1}e^{-b'\\lambda} $$\n",
    "Which has the same form as the prior, indicating that the Gamma family is in fact a conjugate prior for the rate parameter.\n",
    "\n",
    "## c) Show that if $ N $ values of $ Y $ are independent, identically distributed and exponential with rate lambda, then using a Gamma prior on lambda results in a Gamma posterior. Find its parameters.\n",
    "$$ p(\\lambda|y_1, ..., y_N) = \\frac{p(\\lambda) p(y_1, ..., y_N | \\lambda)}{\\int_0^1 p(\\lambda') p(y_1, ..., y_N | \\lambda')} $$\n",
    "To find the posterior, first we must find the joint probability of $ y_1, ..., y_N $ given lambda\n",
    "$$ p(y_1, ..., y_N | \\lambda) = \\prod_{n=1}^N p(y_n | \\lambda) = \\prod_{n=1}^N \\lambda e^{-\\lambda y_n} $$\n",
    "Now we can plug that in\n",
    "$$ p(\\lambda|y_1, ..., y_N) = \\frac{ k_{a, b} \\lambda^{a-1}e^{-b\\lambda} \\prod_{n=1}^N \\lambda e^{-\\lambda y_n}}{\\int_0^1 p(\\lambda') p(y_1, ..., y_N | \\lambda')} = k' \\prod_{n=1}^N \\lambda^a e^{-\\lambda(y_n+b)} =  k' \\lambda^{a' - 1} e^{-b'\\lambda} $$\n",
    "\n",
    "This is a Gamma posterior with $ a' = N*a + 1 $ and $ b' = \\sum_{n=1}^N y_n + b $\n",
    "\n",
    "\n",
    "\n",
    "# 8. Simulating the Posterior for a Poisson Parameter.\n",
    "The number of customers who buy fish tacos at a taqueria in a given hour can be modeled by a Poisson distribution\n",
    "$$ P(k) = \\frac{e^{-\\lambda}\\lambda^k}{k!} $$\n",
    "\n",
    "## a) Find the likelihood function for lambda given a data set with independent customer counts for $ N $ different hours.\n",
    "$$ L(\\lambda; Y_1, ..., Y_N) = p(Y_1, ..., Y_N; \\lambda) = \\prod_{n=1}^N \\frac{e^{-\\lambda}\\lambda^{y_n}}{y_n!} $$\n",
    "\n",
    "## b) Find the MLE of lambda.\n",
    "First we will take the natural log of $ L $\n",
    "$$ ln(L(\\lambda; Y_1, ..., Y_N)) = \\sum_{n=1}^N \\bigg(-\\lambda + y_nln(\\lambda) - ln(y_n!) \\bigg) $$\n",
    "Now we can take the partial derivative of $ ln(L) $ in terms of lambda\n",
    "$$ \\frac{\\partial ln(L)}{\\partial \\lambda} = \\sum_{n=1}^N \\bigg( -1 + \\frac{y_n}{\\lambda} \\bigg) $$\n",
    "We set that equal to zero and solve for lambda\n",
    "$$ 0 = \\sum_{n=1}^N \\bigg( -1 + \\frac{y_n}{\\hat{\\lambda}} \\bigg) $$\n",
    "Add $ N $ to both sides, multiply by lambda, and divide by $ N $ to get\n",
    "$$ \\hat{\\lambda} = \\frac{1}{N} \\sum_{n=1}^N y_n $$\n",
    "\n",
    "## c) Show that the likelihood function for lambda if you take the entire $ N $ hour period as one data point is a constant multiple of the original likelihood function.\n",
    "If we take the entire $ N $ hour period as one data point\n",
    "$$ L(\\lambda; Y) = p(Y; \\lambda) = \\frac{e^{-N\\lambda}(N\\lambda)^{y}}{y!} $$\n",
    "We can now compare that to the original likelihood function and solve for the constant multiple\n",
    "$$ x\\frac{e^{-N\\lambda}(N\\lambda)^{y}}{y!} = \\prod_{n=1}^N \\frac{e^{-\\lambda}\\lambda^{y_n}}{y_n!} $$\n",
    "This will be easier if we take the natural log of both sides first\n",
    "$$ ln(x) - N\\lambda + yln(N\\lambda) - ln(y!) = \\sum_{n=1}^N \\bigg( -\\lambda + y_nln(\\lambda) - ln(y_n!) \\bigg) $$\n",
    "Distributing the sum, and isolating the x on the left, we get\n",
    "$$ ln(x) = \\sum_{n=1}^N y_nln(\\lambda) - \\sum_{n=1}^N ln(y_n!) - yln(N) - yln(\\lambda) + ln(y!) $$\n",
    "Because we summed together all the customers in the $ N $ hour period,\n",
    "$$ y = \\sum_{n=1}^N y_n $$\n",
    "That means terms cancel and we are left with\n",
    "$$ x = N^y $$\n",
    "Which is a constant as we intended to show.\n",
    "\n",
    "The fact that the likelihood function for lambda if you take the entire $ N $ hour period as one data point is a multiple of the original likelihood function indicates that the order of the observations or how they are grouped does not effect the inferred value of lambda. No matter which way lambda is computed, the result condences all of the information about the individual data points into only the essential information.\n",
    "\n",
    "## d) The conjugate prior for the Poisson parameter is a Gamma distribution. Find the posterior parameters in terms of the prior parameters and the data.\n",
    "The posterior distribution of the parameter is\n",
    "$$ p(\\lambda | y_1, ..., y_N) = \\prod_{n=1}^N \\frac{p(\\lambda)p(y_n | \\lambda)}{\\int_0^1 p(\\lambda') p(y_n | \\lambda') } = \\prod_{n=1}^N \\frac{\\frac{b^a}{\\Gamma(a)} \\lambda^{a-1}e^{-b\\lambda} \\frac{e^{-\\lambda}\\lambda^y_n}{y_n!}}{\\int_0^1 p(\\lambda') p(y_n | \\lambda') } $$\n",
    "\n",
    "$$ \\prod_{n=1}^N \\frac{b^a}{\\Gamma(a)} \\frac{1}{\\int_0^1 p(\\lambda') p(y_n | \\lambda')} \\frac{1}{y_n!} $$ is a constant, so we'll call it $ k $ to get it out of the way.\n",
    "We're left with\n",
    "$$ p(\\lambda | y_1, ..., y_N) = k \\prod_{n=1}^N \\lambda^{a-1}e^{-b\\lambda} e^{-\\lambda}\\lambda^y_n = k \\prod_{n=1}^N \\lambda^{a-1+y_n}e^{-\\lambda(b+1)} = k \\prod_{n=1}^N \\lambda^{a'-1}e^{-b'\\lambda} $$\n",
    "Where $ b' = b+1 $ and $ a' = a + y_n $.\n",
    "\n",
    "## e) Write a function that simulates drawing from the distribution of lambda by generating pairs of $ y $ and lambda from their joint distribution and only keeping values of lambda that come with a value of $ y $ that is in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def simdraw(priora, priorb, data, retpairs):\n",
    "    \"\"\" Draws pairs of values of y and lambda from their joint distribution until retpairs of them have a y value\n",
    "    equal to data, using priora and priorb as parameters on the prior distribution of lambda. \"\"\"\n",
    "    \n",
    "    # list of values of lambda that were paired with a value of y equal to data\n",
    "    keptl = []\n",
    "    \n",
    "    # generate values of y and lambda until there are retpairs values in keptl\n",
    "    while len(keptl) < retpairs:\n",
    "        # generate a value of lambda\n",
    "        lamb = numpy.random.gamma(priora, priorb)\n",
    "        # use that to generate a value of y\n",
    "        y = numpy.random.poisson(lamb)\n",
    "        # if the generated value of y is equal to the data, save the value of lambda used to generate it\n",
    "        if y == data:\n",
    "            keptl.append(lamb)\n",
    "    \n",
    "    # once enough values have been found, return the list of lambdas\n",
    "    return keptl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Run the algorithm on a few different combinations of values of $ a $, $ b $, and $ y_N $. Plot the simulated samples and the theoretical posterior density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.integrate as integrate\n",
    "\n",
    "def plotsim(priora, priorb, data, retpairs=20):\n",
    "    \"\"\" Runs simdraw for a given set of parameter values and plots the results against the theoretical posterior density \"\"\"\n",
    "    \n",
    "    # get retpairs simulated values of lambda\n",
    "    samples = simdraw(priora, priorb, data, retpairs)\n",
    "    \n",
    "    # plot them as a histogram\n",
    "    plt.hist(samples)\n",
    "    # add the theoretical posterior density on top\n",
    "    x = numpy.arange(data*2)\n",
    "    y = ((x**(priora-1+data))*(numpy.exp(-x*(priorb+1)))*(priorb**priora)/(math.gamma(priora)*math.factorial(data))\n",
    "         /(integrate.quad(lambda y: posterior(priora, priorb, data, y), 0, data*2)[0]))\n",
    "    plt.plot(x, y, \"r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def posterior(priora, priorb, data, x):\n",
    "    \"\"\" The function for the posterior to integrate from 0 to 1\"\"\"\n",
    "    return (x**(priora-1+data))*(math.exp(-x*(priorb+1)))*(priorb**priora)/(math.gamma(priora)*math.factorial(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Lets try it out with $ a = 1 $, $ b = 1 $, and $ y_N = 5 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwhnUklS4/wc5hVo7uxhMSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8259d73b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotsim(1, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "What about $ a = 2 $, $ b = .5 $ and $ y_n = 5 $? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSYUzqCWpcP8P/Ht/yAtylcQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8259e3f390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotsim(2, .5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or $ a = 5 $, $ b = 5 $ and $ y_n = 25 $?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8259eab2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotsim(5, 5, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced by the results of the simulation with $ a = 1 $, $ b = 1 $ and $ y_n = 5 $, it is possible for the mean of the simulated values of lambda to be close to the actual theoretical mean. However, in most cases that I tried, the simulated mean is far removed from the theoretical mean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
